{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62767c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,json, networkx as nx,requests, matplotlib.pyplot as plt, math, os\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67da670",
   "metadata": {},
   "source": [
    "# Scrape all session report links from French National Assembly (Legislature 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d2f492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(url, output_path):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    print(f\"Fetching {url}...\")\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.encoding = 'utf-8'\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all sections with class 'tabs-content'\n",
    "        content_sections = soup.find_all(class_='tabs-content')\n",
    "        print(f\"Found {len(content_sections)} content sections\")\n",
    "        \n",
    "        all_links = {\"asp\": [], \"folder\": []}\n",
    "        \n",
    "        for section in content_sections:\n",
    "            # Find 'liens-liste' within the section\n",
    "            link_lists = section.find_all(class_='liens-liste')\n",
    "            \n",
    "            for link_list in link_lists:\n",
    "                # Find all links within the list\n",
    "                links = link_list.find_all('a', href=True)\n",
    "                for link in links:\n",
    "                    href = link.get('href')\n",
    "                    if href:\n",
    "                        # Make absolute URL\n",
    "                        full_url = urljoin(url, href)\n",
    "                        \n",
    "                        # Determine key based on extension\n",
    "                        if full_url.endswith('.asp'):\n",
    "                            all_links[\"asp\"].append(full_url)\n",
    "                        else:\n",
    "                            all_links[\"folder\"].append(full_url)\n",
    "        \n",
    "        print(f\"Total links collected: {len(all_links['asp']) + len(all_links['folder'])}\")\n",
    "        \n",
    "        # Save to JSON\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_links, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"Links saved to {output_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb1ba2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder_links(json_path):\n",
    "    # Load existing links\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    folder_links = data.get(\"folder\", [])\n",
    "    asp_links = data.get(\"asp\", [])\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    print(f\"Processing {len(folder_links)} folder links...\")\n",
    "\n",
    "    for folder_url in folder_links:\n",
    "        print(f\"Scraping {folder_url}...\")\n",
    "        try:\n",
    "            response = requests.get(folder_url, headers=headers)\n",
    "            response.encoding = 'utf-8'\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Find all h1 tags with class 'seance' which contain the links\n",
    "                seance_headers = soup.find_all('h1', class_='seance')\n",
    "                \n",
    "                count = 0\n",
    "                for header in seance_headers:\n",
    "                    link = header.find('a', href=True)\n",
    "                    if link:\n",
    "                        href = link.get('href')\n",
    "                        # Construct absolute URL\n",
    "                        full_url = urljoin(folder_url, href)\n",
    "                        \n",
    "                        # Avoid duplicates\n",
    "                        if full_url not in asp_links:\n",
    "                            asp_links.append(full_url)\n",
    "                            count += 1\n",
    "                print(f\"  Found {count} new sessions.\")\n",
    "            else:\n",
    "                print(f\"  Failed to fetch {folder_url}: {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {folder_url}: {e}\")\n",
    "        \n",
    "        # Be nice to the server\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    # Update the data structure\n",
    "    data[\"asp\"] = asp_links\n",
    "    print(f\"Total ASP links: {len(asp_links)}\")\n",
    "\n",
    "    # Save updated JSON\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Updated {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7492bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deputy_mapping(actors_dir):\n",
    "    mapping = {}\n",
    "    files = [f for f in os.listdir(actors_dir) if f.endswith('.json')]\n",
    "    \n",
    "    print(f\"Processing {len(files)} actor files...\")\n",
    "    \n",
    "    for filename in files:\n",
    "        filepath = os.path.join(actors_dir, filename)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            actor = data.get('acteur', {})\n",
    "            uid = actor.get('uid', {}).get('#text')\n",
    "            ident = actor.get('etatCivil', {}).get('ident', {})\n",
    "            \n",
    "            civ = ident.get('civ', '')\n",
    "            prenom = ident.get('prenom', '')\n",
    "            nom = ident.get('nom', '')\n",
    "            \n",
    "            full_name = f\"{civ} {prenom} {nom}\".strip()\n",
    "            \n",
    "            if uid and full_name:\n",
    "                mapping[full_name] = uid\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            \n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_speeches(json_path, deputees_json_path, deputy_mapping):\n",
    "    # Load links\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Load existing deputees data\n",
    "    with open(deputees_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        deputees_data = json.load(f)\n",
    "        \n",
    "    asp_links = data.get(\"asp\", [])\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    print(f\"Starting scrape of {len(asp_links)} sessions...\")\n",
    "    \n",
    "    for i, url in enumerate(asp_links):\n",
    "        print(f\"Processing {i}/{len(asp_links)}: {url} \\r\")\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                response.encoding = 'utf-8'\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                current_speaker_id = None\n",
    "                \n",
    "                for p in soup.find_all('p'):\n",
    "                    if p.get('class'):\n",
    "                        continue\n",
    "                    \n",
    "                    b_tag = p.find('b')\n",
    "                    \n",
    "                    if b_tag:\n",
    "                        b_text = b_tag.get_text(strip=True)\n",
    "                        p_text = p.get_text(strip=True)\n",
    "                        \n",
    "                        if p_text.startswith(b_text):\n",
    "                            # Clean speaker name\n",
    "                            raw_name = b_text.strip(\" .:,\")\n",
    "                            \n",
    "                            # Try to find ID in mapping\n",
    "                            # The mapping keys are like \"M. Jean-Marc Ayrault\"\n",
    "                            # The raw_name might be \"M. Jean-Marc Ayrault\" or similar\n",
    "                            if raw_name in deputy_mapping:\n",
    "                                current_speaker_id = deputy_mapping[raw_name]\n",
    "                            else:\n",
    "                                # Try fuzzy match or partial match if needed, or skip\n",
    "                                # For now, strict match or skip\n",
    "                                current_speaker_id = None\n",
    "                                # print(f\"Unknown speaker: {raw_name}\") \n",
    "\n",
    "                            b_tag.decompose()\n",
    "                    \n",
    "                    if current_speaker_id and current_speaker_id in deputees_data:\n",
    "                        for tag in p.find_all(['i', 'b']):\n",
    "                            tag.decompose()\n",
    "                        \n",
    "                        text = p.get_text(strip=True)\n",
    "                        \n",
    "                        # Clean leading punctuation\n",
    "                        #text = text.lstrip(\" .,; ,.\")\n",
    "                        \n",
    "                        if text:\n",
    "                            # Add speech to the specific deputy\n",
    "                            deputees_data[current_speaker_id][\"speeches\"].append(text)\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "    # Save the updated deputees data\n",
    "    with open(deputees_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(deputees_data, f, ensure_ascii=False, indent=1)\n",
    "        \n",
    "    print(f\"Scraping complete. Updated {deputees_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "843bfa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://www.assemblee-nationale.fr/14/debats/index.asp...\n",
      "Found 6 content sections\n",
      "Total links collected: 19\n",
      "Links saved to ./data/scraping/links/session_links_14.json\n",
      "Processing 18 folder links...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2016-2017/...\n",
      "Found 6 content sections\n",
      "Total links collected: 19\n",
      "Links saved to ./data/scraping/links/session_links_14.json\n",
      "Processing 18 folder links...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2016-2017/...\n",
      "  Found 123 new sessions.\n",
      "  Found 123 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2015-2016/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2015-2016/...\n",
      "  Found 232 new sessions.\n",
      "  Found 232 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2015-2016-extra/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2015-2016-extra/...\n",
      "  Found 18 new sessions.\n",
      "  Found 18 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2015-2016-extra2/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2015-2016-extra2/...\n",
      "  Found 6 new sessions.\n",
      "  Found 6 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2014-2015/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2014-2015/...\n",
      "  Found 275 new sessions.\n",
      "  Found 275 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2014-2015-extra/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2014-2015-extra/...\n",
      "  Found 27 new sessions.\n",
      "  Found 27 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2014-2015-extra2/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2014-2015-extra2/...\n",
      "  Found 12 new sessions.\n",
      "  Found 12 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2013-2014/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2013-2014/...\n",
      "  Found 256 new sessions.\n",
      "  Found 256 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2013-2014-extra/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2013-2014-extra/...\n",
      "  Found 27 new sessions.\n",
      "  Found 27 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2013-2014-extra2/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2013-2014-extra2/...\n",
      "  Found 14 new sessions.\n",
      "  Found 14 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2013-2014-extra3/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2013-2014-extra3/...\n",
      "  Found 1 new sessions.\n",
      "  Found 1 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2012-2013/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2012-2013/...\n",
      "  Found 290 new sessions.\n",
      "  Found 290 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2012-2013-extra/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2012-2013-extra/...\n",
      "  Found 31 new sessions.\n",
      "  Found 31 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2012-2013-extra2/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2012-2013-extra2/...\n",
      "  Found 1 new sessions.\n",
      "  Found 1 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2012-2013-extra3/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2012-2013-extra3/...\n",
      "  Found 15 new sessions.\n",
      "  Found 15 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2011-2012/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2011-2012/...\n",
      "  Found 2 new sessions.\n",
      "  Found 2 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2011-2012-extra/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2011-2012-extra/...\n",
      "  Found 18 new sessions.\n",
      "  Found 18 new sessions.\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2011-2012-extra2/...\n",
      "Scraping https://www.assemblee-nationale.fr/14/cri/2011-2012-extra2/...\n",
      "  Found 10 new sessions.\n",
      "  Found 10 new sessions.\n",
      "Total ASP links: 1359\n",
      "Updated ./data/scraping/links/session_links_14.json\n",
      "Processing 3082 actor files...\n",
      "Total ASP links: 1359\n",
      "Updated ./data/scraping/links/session_links_14.json\n",
      "Processing 3082 actor files...\n",
      "Starting scrape of 1359 sessions...\n",
      "Starting scrape of 1359 sessions...\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2016-2017/20170121.asp: Response ended prematurely\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2016-2017/20170121.asp: Response ended prematurely\n",
      "Processing 50/1359: https://www.assemblee-nationale.fr/14/cri/2016-2017/20170073.asp\n",
      "Processing 50/1359: https://www.assemblee-nationale.fr/14/cri/2016-2017/20170073.asp\n",
      "Scraping complete. Updated ./data/processed/deputees_14.json\n",
      "Scraping complete. Updated ./data/processed/deputees_14.json\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.assemblee-nationale.fr/14/debats/index.asp\"\n",
    "links_file = \"./data/scraping/links/session_links_14.json\"\n",
    "deputees_json = \"./data/processed/deputees_14.json\"\n",
    "\n",
    "scrape_links(base_url, links_file)\n",
    "process_folder_links(links_file)\n",
    "deputy_map = create_deputy_mapping(\"./data/all_actors/acteur\")\n",
    "scrape_all_speeches(links_file, deputees_json, deputy_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4359ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scrape of 1359 sessions...\n",
      "Processing 50/1359: https://www.assemblee-nationale.fr/14/cri/2016-2017/20170073.asp\n",
      "Processing 50/1359: https://www.assemblee-nationale.fr/14/cri/2016-2017/20170073.asp\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2016-2017/20170028.asp: Response ended prematurely\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2016-2017/20170028.asp: Response ended prematurely\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2016-2017/20170029.asp: Response ended prematurely\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2016-2017/20170029.asp: Response ended prematurely\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2016-2017/20170024.asp: HTTPSConnectionPool(host='www.assemblee-nationale.fr', port=443): Max retries exceeded with url: /14/cri/2016-2017/20170024.asp (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000002820B3AB1D0>, 'Connection to www.assemblee-nationale.fr timed out. (connect timeout=None)'))\n",
      "Processing 100/1359: https://www.assemblee-nationale.fr/14/cri/2016-2017/20170025.asp\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2016-2017/20170024.asp: HTTPSConnectionPool(host='www.assemblee-nationale.fr', port=443): Max retries exceeded with url: /14/cri/2016-2017/20170024.asp (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000002820B3AB1D0>, 'Connection to www.assemblee-nationale.fr timed out. (connect timeout=None)'))\n",
      "Processing 100/1359: https://www.assemblee-nationale.fr/14/cri/2016-2017/20170025.asp\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2016-2017/20170022.asp: HTTPSConnectionPool(host='www.assemblee-nationale.fr', port=443): Max retries exceeded with url: /14/cri/2016-2017/20170022.asp (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000028202938260>, 'Connection to www.assemblee-nationale.fr timed out. (connect timeout=None)'))\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2016-2017/20170022.asp: HTTPSConnectionPool(host='www.assemblee-nationale.fr', port=443): Max retries exceeded with url: /14/cri/2016-2017/20170022.asp (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000028202938260>, 'Connection to www.assemblee-nationale.fr timed out. (connect timeout=None)'))\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2015-2016/20160218.asp: Response ended prematurely\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2015-2016/20160218.asp: Response ended prematurely\n",
      "Processing 150/1359: https://www.assemblee-nationale.fr/14/cri/2015-2016/20160205.asp\n",
      "Processing 150/1359: https://www.assemblee-nationale.fr/14/cri/2015-2016/20160205.asp\n",
      "Processing 200/1359: https://www.assemblee-nationale.fr/14/cri/2015-2016/20160157.asp\n",
      "Processing 200/1359: https://www.assemblee-nationale.fr/14/cri/2015-2016/20160157.asp\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2015-2016/20160108.asp: Response ended prematurely\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2015-2016/20160108.asp: Response ended prematurely\n",
      "Processing 250/1359: https://www.assemblee-nationale.fr/14/cri/2015-2016/20160105.asp\n",
      "Processing 250/1359: https://www.assemblee-nationale.fr/14/cri/2015-2016/20160105.asp\n",
      "Processing 300/1359: https://www.assemblee-nationale.fr/14/cri/2015-2016/20160057.asp\n",
      "Processing 300/1359: https://www.assemblee-nationale.fr/14/cri/2015-2016/20160057.asp\n",
      "Processing 350/1359: https://www.assemblee-nationale.fr/14/cri/2015-2016/20160006.asp\n",
      "Processing 350/1359: https://www.assemblee-nationale.fr/14/cri/2015-2016/20160006.asp\n",
      "Processing 400/1359: https://www.assemblee-nationale.fr/14/cri/2014-2015/20150256.asp\n",
      "Processing 400/1359: https://www.assemblee-nationale.fr/14/cri/2014-2015/20150256.asp\n",
      "Processing 450/1359: https://www.assemblee-nationale.fr/14/cri/2014-2015/20150204.asp\n",
      "Processing 450/1359: https://www.assemblee-nationale.fr/14/cri/2014-2015/20150204.asp\n",
      "Processing 500/1359: https://www.assemblee-nationale.fr/14/cri/2014-2015/20150156.asp\n",
      "Processing 500/1359: https://www.assemblee-nationale.fr/14/cri/2014-2015/20150156.asp\n",
      "Processing 550/1359: https://www.assemblee-nationale.fr/14/cri/2014-2015/20150105.asp\n",
      "Processing 550/1359: https://www.assemblee-nationale.fr/14/cri/2014-2015/20150105.asp\n",
      "Processing 600/1359: https://www.assemblee-nationale.fr/14/cri/2014-2015/20150054.asp\n",
      "Processing 600/1359: https://www.assemblee-nationale.fr/14/cri/2014-2015/20150054.asp\n",
      "Processing 650/1359: https://www.assemblee-nationale.fr/14/cri/2014-2015/20150004.asp\n",
      "Processing 650/1359: https://www.assemblee-nationale.fr/14/cri/2014-2015/20150004.asp\n",
      "Processing 700/1359: https://www.assemblee-nationale.fr/14/cri/2013-2014/20140252.asp\n",
      "Processing 700/1359: https://www.assemblee-nationale.fr/14/cri/2013-2014/20140252.asp\n",
      "Processing 750/1359: https://www.assemblee-nationale.fr/14/cri/2013-2014/20140201.asp\n",
      "Processing 750/1359: https://www.assemblee-nationale.fr/14/cri/2013-2014/20140201.asp\n",
      "Processing 800/1359: https://www.assemblee-nationale.fr/14/cri/2013-2014/20140152.asp\n",
      "Processing 800/1359: https://www.assemblee-nationale.fr/14/cri/2013-2014/20140152.asp\n",
      "Processing 850/1359: https://www.assemblee-nationale.fr/14/cri/2013-2014/20140099.asp\n",
      "Processing 850/1359: https://www.assemblee-nationale.fr/14/cri/2013-2014/20140099.asp\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2013-2014/20140073.asp: Response ended prematurely\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2013-2014/20140073.asp: Response ended prematurely\n",
      "Processing 900/1359: https://www.assemblee-nationale.fr/14/cri/2013-2014/20140049.asp\n",
      "Processing 900/1359: https://www.assemblee-nationale.fr/14/cri/2013-2014/20140049.asp\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2013-2014/20140031.asp: Response ended prematurely\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2013-2014/20140031.asp: Response ended prematurely\n",
      "Processing 950/1359: https://www.assemblee-nationale.fr/14/cri/2013-2014-extra/20141027.asp\n",
      "Processing 950/1359: https://www.assemblee-nationale.fr/14/cri/2013-2014-extra/20141027.asp\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2013-2014-extra/20141012.asp: Response ended prematurely\n",
      "Error scraping https://www.assemblee-nationale.fr/14/cri/2013-2014-extra/20141012.asp: Response ended prematurely\n",
      "Processing 1000/1359: https://www.assemblee-nationale.fr/14/cri/2012-2013/20130281.asp\n",
      "Processing 1000/1359: https://www.assemblee-nationale.fr/14/cri/2012-2013/20130281.asp\n",
      "Processing 1050/1359: https://www.assemblee-nationale.fr/14/cri/2012-2013/20130230.asp\n",
      "Processing 1050/1359: https://www.assemblee-nationale.fr/14/cri/2012-2013/20130230.asp\n",
      "Processing 1100/1359: https://www.assemblee-nationale.fr/14/cri/2012-2013/20130183.asp\n",
      "Processing 1100/1359: https://www.assemblee-nationale.fr/14/cri/2012-2013/20130183.asp\n",
      "Processing 1150/1359: https://www.assemblee-nationale.fr/14/cri/2012-2013/20130130.asp\n",
      "Processing 1150/1359: https://www.assemblee-nationale.fr/14/cri/2012-2013/20130130.asp\n",
      "Processing 1200/1359: https://www.assemblee-nationale.fr/14/cri/2012-2013/20130081.asp\n",
      "Processing 1200/1359: https://www.assemblee-nationale.fr/14/cri/2012-2013/20130081.asp\n",
      "Processing 1250/1359: https://www.assemblee-nationale.fr/14/cri/2012-2013/20130033.asp\n",
      "Processing 1250/1359: https://www.assemblee-nationale.fr/14/cri/2012-2013/20130033.asp\n",
      "Processing 1300/1359: https://www.assemblee-nationale.fr/14/cri/2012-2013-extra/20131012.asp\n",
      "Processing 1300/1359: https://www.assemblee-nationale.fr/14/cri/2012-2013-extra/20131012.asp\n",
      "Processing 1350/1359: https://www.assemblee-nationale.fr/14/cri/2011-2012-extra2/20122010.asp\n",
      "Processing 1350/1359: https://www.assemblee-nationale.fr/14/cri/2011-2012-extra2/20122010.asp\n"
     ]
    }
   ],
   "source": [
    "scrape_all_speeches(links_file, deputees_json, deputy_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c2ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
