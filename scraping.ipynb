{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62767c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,json, networkx as nx,requests, matplotlib.pyplot as plt, math, os\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67da670",
   "metadata": {},
   "source": [
    "# Scrape all session report links from French National Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d2f492a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Fetching page 4...\n",
      "Fetching page 5...\n",
      "Fetching page 5...\n",
      "Fetching page 6...\n",
      "Fetching page 6...\n",
      "Fetching page 7...\n",
      "Fetching page 7...\n",
      "Fetching page 8...\n",
      "Fetching page 8...\n",
      "Fetching page 9...\n",
      "Fetching page 9...\n",
      "Fetching page 10...\n",
      "Fetching page 10...\n",
      "Fetching page 11...\n",
      "Fetching page 11...\n",
      "\n",
      "Total links collected: 346\n",
      "\n",
      "Total links collected: 346\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.assemblee-nationale.fr/dyn/17/comptes-rendus/seance\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "all_links = []\n",
    "page = 1\n",
    "\n",
    "while True:\n",
    "    url = f\"{base_url}?limit=36&page={page}\"\n",
    "    print(f\"Fetching page {page}...\")\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.encoding = 'utf-8'\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        break\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    session_blocks = soup.find_all('div', class_='an-bloc _style-crs')\n",
    "    \n",
    "    if not session_blocks:\n",
    "        break\n",
    "    \n",
    "    for block in session_blocks:\n",
    "        links = block.find_all('a', class_='button')\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            # Only keep HTML links (not PDF)\n",
    "            if href and not href.endswith('.pdf'):\n",
    "                all_links.append(href)\n",
    "    \n",
    "    page += 1\n",
    "    time.sleep(0)\n",
    "\n",
    "print(f\"\\nTotal links collected: {len(all_links)}\")\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"./data/scraping/links/session_links_17.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_links, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9374e33e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
