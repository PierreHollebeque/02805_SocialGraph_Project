{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "748f7aa5",
   "metadata": {},
   "source": [
    "From the list of cr created from the reunion, this file will create a json listing of every deputies with their most frequent words and stopwords removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3104758a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\remib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\remib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, os, sys, warnings, math, requests, re, random, io\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Union\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ac7d1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CRSANR5L17S2024D1N001...\n",
      "@attributes\n",
      "@attributes\n",
      "@attributes\n",
      "@attributes\n",
      "@attributes\n",
      "@attributes\n",
      "PA721908 Mes chers collègues, c’est avec une immense émotion que je p ...\n",
      "PA791812 Ils n’ont pas voté pour vous ! \n",
      "             ...\n",
      "PA721908 Ces voix, ces suffrages, cette mobilisation exceptionnelle e ...\n",
      "PA721410 Très bien ! ...\n",
      "PA721908 …des déserts médicaux, de nos écoles, de nos services public ...\n",
      "PA719118 Oui ! ...\n",
      "PA721908 …de notre défense ». Quels que soient nos bords politiques e ...\n",
      "PA795010 Mais oui, bien sûr ! ...\n",
      "PA721908 Cette Assemblée nationale, sans doute plus représentative qu ...\n",
      "PA795010 Cela a bien marché jusqu’ici… ...\n",
      "PA721908 Cette élection m’oblige plus que jamais, davantage que celle ...\n",
      "PA736201 Nous avons déjà entendu ce discours ! ...\n"
     ]
    }
   ],
   "source": [
    "processed_file_name = 'data/speeches_processed.json'\n",
    "dir_report_reunions = 'data/cr/'\n",
    "\n",
    "# Load previously processed data if it exists\n",
    "if os.path.exists(processed_file_name):\n",
    "    with open(processed_file_name,'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        speeches = data['speeches']\n",
    "        processed_files = data['processed_files']\n",
    "else:\n",
    "    speeches = {}\n",
    "    processed_files = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(dir_report_reunions):\n",
    "    if filename.endswith('.json') and filename.removesuffix('.json') not in processed_files: #No processed yet\n",
    "        print(f\"Processing {filename.removesuffix('.json')}...\")\n",
    "        with open(os.path.join(dir_report_reunions, filename), 'r', encoding='utf-8') as f:\n",
    "            report = json.loads(f.read())\n",
    "\n",
    "\n",
    "        for speech_0 in report['contenu']['point']:\n",
    "            try :\n",
    "                for speech in speech_0['paragraphe']:\n",
    "                    speech_person_id = speech['@attributes']['id_acteur']\n",
    "                    speech_text = speech['texte']\n",
    "                    print(speech_person_id,speech_text[:60],'...')\n",
    "            except :\n",
    "                print(speech)\n",
    "\n",
    "        break\n",
    "        processed_files.append(filename.removesuffix('.json'))\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "# data = {\n",
    "#     'speeches': speeches,\n",
    "#     'processed_files': processed_files\n",
    "# }\n",
    "# with open(processed_file_name,'w', encoding='utf-8') as f:\n",
    "#     json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
